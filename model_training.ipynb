{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b02331",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install XGBoost if you haven't already (uncomment if needed)\n",
    "# !pip install xgboost\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"--- Configuring Model Training ---\")\n",
    "\n",
    "# 1. DEFINE LEAKY COLUMNS TO DROP\n",
    "# These are the IDs causing the model to \"memorize\" instead of learn\n",
    "id_cols_to_drop = [\n",
    "    'Request ID', \n",
    "    'Case Id', \n",
    "    'Service Provider Id', \n",
    "    'Service Provider Contact Id',\n",
    "    'Claim ID' # Just in case\n",
    "]\n",
    "\n",
    "# 2. PREPARE X and y\n",
    "target_name = 'Wages Reliability'\n",
    "targets_to_exclude = ['Annual Leave Reliability', 'Long Service Leave Reliability', 'Wages Reliability']\n",
    "\n",
    "# Drop Targets AND the IDs\n",
    "cols_to_drop = targets_to_exclude + id_cols_to_drop\n",
    "# Only drop what actually exists\n",
    "existing_drop = [c for c in cols_to_drop if c in df.columns]\n",
    "\n",
    "X = df.drop(columns=existing_drop)\n",
    "y = df[target_name]\n",
    "\n",
    "# 3. SPLIT DATA\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training Features: {X_train.shape[1]} columns\")\n",
    "print(f\"Training Rows: {X_train.shape[0]}\")\n",
    "\n",
    "# 4. TRAIN XGBOOST (The Upgrade)\n",
    "# n_estimators=500: More trees\n",
    "# learning_rate=0.05: Learns slower but more accurately\n",
    "# n_jobs=-1: Uses all CPU cores\n",
    "model = XGBRegressor(\n",
    "    n_estimators=500, \n",
    "    learning_rate=0.05, \n",
    "    max_depth=6, \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost... (This might take 30 seconds)\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. EVALUATE\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n--- Final Model Performance ---\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R2 Score (Accuracy):       {r2:.4f}\")\n",
    "\n",
    "# 6. PLOT FEATURE IMPORTANCE (The \"Clean\" View)\n",
    "# Now that IDs are gone, what REALLY matters?\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance, palette='magma')\n",
    "plt.title(f'True Drivers of {target_name} (IDs Removed)')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7fe85c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. SELECT FEATURES & TARGET\n",
    "# The Target is what we want to predict\n",
    "target_name = 'Wages Reliability'\n",
    "\n",
    "# The Features (X) are everything EXCEPT the target columns\n",
    "# We drop ALL reliability targets so the model doesn't \"cheat\" by looking at Annual Leave to predict Wages\n",
    "drop_targets = ['Annual Leave Reliability', 'Long Service Leave Reliability', 'Wages Reliability']\n",
    "X = df.drop(columns=drop_targets, errors='ignore')\n",
    "y = df[target_name]\n",
    "\n",
    "# 2. SPLIT DATA (80% Training, 20% Testing)\n",
    "# random_state=42 ensures we get the same split every time (reproducibility)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training on {X_train.shape[0]} rows. Testing on {X_test.shape[0]} rows.\")\n",
    "\n",
    "# 3. TRAIN THE MODEL\n",
    "# n_estimators=100 means \"build 100 decision trees\"\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úÖ Model Training Complete.\")\n",
    "\n",
    "# 4. EVALUATE PERFORMANCE\n",
    "# Make predictions on the Test Set (data the model has never seen)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n--- Model Performance Results ---\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R2 Score (Accuracy):       {r2:.4f}\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Interpretation: On average, the model's reliability score is off by {mae*100:.2f}%.\")\n",
    "\n",
    "# 5. VISUALIZE FEATURE IMPORTANCE (The \"Why\")\n",
    "# This shows which columns drove the decisions the most\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance, palette='viridis')\n",
    "plt.title(f'Top 10 Predictors for {target_name}')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2876d8a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Diagnostic Cell: Where did the features go?\n",
    "\n",
    "print(\"--- Checking for Key Features in Training Data ---\")\n",
    "\n",
    "# List of the 'Smart' features we expect to see\n",
    "key_features = [\n",
    "    'Provider_Reputation_Score', \n",
    "    'IP_Wage_to_ABS_Ratio', \n",
    "    'IP_Tenure_Years', \n",
    "    'IP Wages'\n",
    "]\n",
    "\n",
    "# 1. Do they exist?\n",
    "for col in key_features:\n",
    "    if col in X_train.columns:\n",
    "        print(f\"‚úÖ FOUND: {col}\")\n",
    "    else:\n",
    "        print(f\"‚ùå MISSING: {col} (This is why it's not in the chart!)\")\n",
    "\n",
    "# 2. If they exist, how weak are they?\n",
    "# Let's look at their correlation with the Target\n",
    "if 'Wages Reliability' in df.columns:\n",
    "    print(\"\\n--- Correlation with Target ---\")\n",
    "    # We check the original df because X_train is already split\n",
    "    for col in key_features:\n",
    "        if col in df.columns:\n",
    "            corr = df[col].corr(df['Wages Reliability'])\n",
    "            print(f\"{col}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144215e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "print(\"--- Pivoting to Binary Classification ---\")\n",
    "\n",
    "# 1. DEFINE THE THRESHOLD (Business Logic)\n",
    "# User Proposal: Anything below 0.95 is \"Unreliable\" (Risk)\n",
    "THRESHOLD = 0.95\n",
    "\n",
    "# Create Binary Target: 1 = RISK (Unreliable), 0 = SAFE (Reliable)\n",
    "# We usually define the \"Positive Class\" (1) as the thing we want to catch (The Risk)\n",
    "y_class = (df['Wages Reliability'] < THRESHOLD).astype(int)\n",
    "\n",
    "print(f\"Reliability Threshold: {THRESHOLD*100}%\")\n",
    "print(\"Class Distribution:\")\n",
    "print(y_class.value_counts(normalize=True).rename({0: 'Safe (Majority)', 1: 'Risk (Minority)'}))\n",
    "\n",
    "# 2. SPLIT DATA\n",
    "# We use the same X features as before (IDs removed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_class, test_size=0.2, random_state=42, stratify=y_class)\n",
    "# Note: stratify=y_class ensures we have the same % of bad cases in train and test\n",
    "\n",
    "# 3. CALCULATE SCALE_POS_WEIGHT\n",
    "# This tells XGBoost: \"Pay X times more attention to the minority class\"\n",
    "# Formula: Count(Majority) / Count(Minority)\n",
    "ratio = float(np.sum(y_train == 0)) / np.sum(y_train == 1)\n",
    "print(f\"\\nImbalance Ratio: {ratio:.2f} (Model will weight 'Risk' cases {ratio:.2f}x more)\")\n",
    "\n",
    "# 4. TRAIN CLASSIFIER\n",
    "model_class = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    scale_pos_weight=ratio, # <--- CRITICAL FIX FOR IMBALANCE\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "model_class.fit(X_train, y_train)\n",
    "\n",
    "# 5. EVALUATE (Confusion Matrix)\n",
    "y_pred_class = model_class.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred_class, target_names=['Safe', 'Risk']))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(6, 5))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_class, display_labels=['Safe', 'Risk'], cmap='Blues', colorbar=False)\n",
    "plt.title(\"Confusion Matrix (Did we catch the risks?)\")\n",
    "plt.show()\n",
    "\n",
    "# 6. CHECK FEATURE IMPORTANCE (Again)\n",
    "# Does the logic change when looking for anomalies?\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model_class.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance, palette='coolwarm')\n",
    "plt.title(f'Top Predictors of UNRELIABILITY (< {THRESHOLD})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2cd5a7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# 1. Get the Probability Scores (0% to 100% risk) instead of just Yes/No\n",
    "y_probs = model_class.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2. Test different Thresholds\n",
    "thresholds = [0.50, 0.60, 0.70, 0.80, 0.85, 0.90, 0.95]\n",
    "\n",
    "print(f\"{'Threshold':<10} | {'Precision (Accuracy of Flags)':<30} | {'Recall (Risks Caught)':<25}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for t in thresholds:\n",
    "    # Apply the new threshold\n",
    "    y_pred_new = (y_probs >= t).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    report = classification_report(y_test, y_pred_new, output_dict=True)\n",
    "    prec = report['1']['precision']\n",
    "    rec = report['1']['recall']\n",
    "    \n",
    "    print(f\"{t:.2f}       | {prec:.2%}                         | {rec:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4605c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visual Proof of \"No Leakage\"\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# We sample 2000 points just to keep the plot readable\n",
    "sns.scatterplot(data=df.sample(2000, random_state=42), x='IP Wages', y='Wages Reliability', alpha=0.3)\n",
    "\n",
    "plt.title(\"Leakage Test: IP Wage vs. Reliability\")\n",
    "plt.xlabel(\"IP Wage ($)\")\n",
    "plt.ylabel(\"Reliability Score (0.0 - 1.0)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40478a3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visual Proof of \"No Leakage\"\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# We sample 2000 points just to keep the plot readable\n",
    "sns.scatterplot(data=df.sample(2000, random_state=42), x='IP Wages', y='Wages Reliability', alpha=0.3)\n",
    "\n",
    "plt.title(\"Leakage Test: IP Wage vs. Reliability\")\n",
    "plt.xlabel(\"IP Wage ($)\")\n",
    "plt.ylabel(\"Reliability Score (0.0 - 1.0)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60803cab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "print(\"--- Sanitizing Column Names for LightGBM ---\")\n",
    "\n",
    "def clean_col_names(df):\n",
    "    # Regex: Replace anything that is NOT (^) a letter, number, or underscore\n",
    "    # with an underscore.\n",
    "    new_columns = [re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in df.columns]\n",
    "    df.columns = new_columns\n",
    "    return df\n",
    "\n",
    "# Apply to Train and Test sets\n",
    "X_train = clean_col_names(X_train)\n",
    "X_test = clean_col_names(X_test)\n",
    "\n",
    "print(\"‚úÖ Columns sanitized. Example of new names:\")\n",
    "print(X_train.columns[:5].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22a6e9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Install LightGBM (if needed)\n",
    "# !pip install lightgbm\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"--- Benchmarking LightGBM ---\")\n",
    "\n",
    "# Calculate the scale_pos_weight equivalent for LightGBM\n",
    "# LightGBM uses 'scale_pos_weight' just like XGBoost\n",
    "ratio = float(np.sum(y_train == 0)) / np.sum(y_train == 1)\n",
    "\n",
    "# 2. Configure Model\n",
    "lgbm = LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    scale_pos_weight=ratio,  # Handle Imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1 # Silences warnings\n",
    ")\n",
    "\n",
    "# 3. Train\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "# 4. Evaluate\n",
    "y_pred_lgbm = lgbm.predict(X_test)\n",
    "\n",
    "print(\"\\n--- LightGBM Results ---\")\n",
    "print(classification_report(y_test, y_pred_lgbm, target_names=['Safe', 'Risk']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8666d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"--- Benchmarking Logistic Regression (The Baseline) ---\")\n",
    "\n",
    "# 1. Logistic Regression requires Scaling (Standardizing inputs)\n",
    "# XGBoost didn't care, but Logistic Regression breaks if Wages are 5000 and Ratio is 1.2\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. Configure Model\n",
    "# class_weight='balanced' is the Logistic equivalent of scale_pos_weight\n",
    "log_reg = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
    "\n",
    "# 3. Train\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4. Evaluate\n",
    "y_pred_log = log_reg.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n--- Logistic Regression Results ---\")\n",
    "print(classification_report(y_test, y_pred_log, target_names=['Safe', 'Risk']))\n",
    "\n",
    "# 5. \"Coefficients\" (The Explainer)\n",
    "# In Logistic Regression, we don't get \"Importance\", we get \"Coefficients\" (Weights)\n",
    "# Positive Coeff = Increases Risk. Negative Coeff = Decreases Risk.\n",
    "coeffs = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': log_reg.coef_[0]\n",
    "}).sort_values('Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\n--- Top Risk Drivers (Logistic) ---\")\n",
    "print(coeffs.head(5))\n",
    "print(\"\\n--- Top Safety Drivers (Logistic) ---\")\n",
    "print(coeffs.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf7bce5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"--- Training Single Decision Tree for Visualization ---\")\n",
    "\n",
    "# 1. Train a shallow tree (max_depth=3)\n",
    "# We keep it shallow so the diagram is readable by humans.\n",
    "# We use class_weight='balanced' to handle the risk imbalance.\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    max_depth=3, \n",
    "    class_weight='balanced', \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# 2. Evaluate (Just to see how it compares)\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "print(\"\\n--- Single Tree Performance ---\")\n",
    "print(classification_report(y_test, y_pred_dt, target_names=['Safe', 'Risk']))\n",
    "\n",
    "# 3. PLOT THE LOGIC\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    dt_model, \n",
    "    feature_names=X.columns,  \n",
    "    class_names=['Safe', 'Risk'],\n",
    "    filled=True,             # Color the boxes (Blue = Risk, Orange = Safe)\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(\"The Business Logic Flowchart (Top 3 Levels)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c633313a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, recall_score, precision_score, f1_score\n",
    "\n",
    "print(\"--- STARTING GRAND MODEL TOURNAMENT ---\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATA PREP (Sanitization for LightGBM)\n",
    "# ==========================================\n",
    "def clean_col_names(df):\n",
    "    return df.rename(columns=lambda x: re.sub(r'[^A-Za-z0-9_]+', '_', x))\n",
    "\n",
    "X_train = clean_col_names(X_train)\n",
    "X_test = clean_col_names(X_test)\n",
    "\n",
    "# Calculate Imbalance Ratio for Boosting Models\n",
    "ratio = float(np.sum(y_train == 0)) / np.sum(y_train == 1)\n",
    "print(f\"Imbalance Ratio: {ratio:.2f}x\")\n",
    "\n",
    "# Store results here\n",
    "leaderboard = []\n",
    "\n",
    "# ==========================================\n",
    "# 2. XGBOOST (The Favorite) - Multi-Threshold\n",
    "# ==========================================\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model = XGBClassifier(n_estimators=500, learning_rate=0.05, max_depth=6, \n",
    "                          scale_pos_weight=ratio, random_state=42, n_jobs=-1, verbosity=0)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_probs = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Test 3 Thresholds\n",
    "for thresh in [0.50, 0.75, 0.90]:\n",
    "    preds = (xgb_probs >= thresh).astype(int)\n",
    "    leaderboard.append({\n",
    "        'Model': f'XGBoost (Threshold {thresh})',\n",
    "        'Recall (Catch Rate)': recall_score(y_test, preds),\n",
    "        'Precision (Accuracy)': precision_score(y_test, preds),\n",
    "        'F1-Score': f1_score(y_test, preds)\n",
    "    })\n",
    "\n",
    "# ==========================================\n",
    "# 3. LIGHTGBM (The Speedster)\n",
    "# ==========================================\n",
    "print(\"Training LightGBM...\")\n",
    "lgbm_model = LGBMClassifier(n_estimators=500, learning_rate=0.05, max_depth=6, \n",
    "                            scale_pos_weight=ratio, random_state=42, n_jobs=-1, verbose=-1)\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "lgbm_preds = lgbm_model.predict(X_test)\n",
    "\n",
    "leaderboard.append({\n",
    "    'Model': 'LightGBM (Default)',\n",
    "    'Recall (Catch Rate)': recall_score(y_test, lgbm_preds),\n",
    "    'Precision (Accuracy)': precision_score(y_test, lgbm_preds),\n",
    "    'F1-Score': f1_score(y_test, lgbm_preds)\n",
    "})\n",
    "\n",
    "# ==========================================\n",
    "# 4. DECISION TREE (The White Box)\n",
    "# ==========================================\n",
    "print(\"Training Decision Tree...\")\n",
    "dt_model = DecisionTreeClassifier(max_depth=4, class_weight='balanced', random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_preds = dt_model.predict(X_test)\n",
    "\n",
    "leaderboard.append({\n",
    "    'Model': 'Decision Tree (Depth 4)',\n",
    "    'Recall (Catch Rate)': recall_score(y_test, dt_preds),\n",
    "    'Precision (Accuracy)': precision_score(y_test, dt_preds),\n",
    "    'F1-Score': f1_score(y_test, dt_preds)\n",
    "})\n",
    "\n",
    "# ==========================================\n",
    "# 5. LOGISTIC REGRESSION (The Baseline)\n",
    "# ==========================================\n",
    "print(\"Training Logistic Regression...\")\n",
    "# Needs scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "log_model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "log_model.fit(X_train_scaled, y_train)\n",
    "log_preds = log_model.predict(X_test_scaled)\n",
    "\n",
    "leaderboard.append({\n",
    "    'Model': 'Logistic Regression',\n",
    "    'Recall (Catch Rate)': recall_score(y_test, log_preds),\n",
    "    'Precision (Accuracy)': precision_score(y_test, log_preds),\n",
    "    'F1-Score': f1_score(y_test, log_preds)\n",
    "})\n",
    "\n",
    "# ==========================================\n",
    "# 6. FINAL RESULTS\n",
    "# ==========================================\n",
    "df_results = pd.DataFrame(leaderboard).sort_values('Recall (Catch Rate)', ascending=False)\n",
    "\n",
    "print(\"\\n--- üèÜ MODEL LEADERBOARD üèÜ ---\")\n",
    "# Display with nice formatting\n",
    "display(df_results.style.background_gradient(cmap='Greens', subset=['Recall (Catch Rate)', 'Precision (Accuracy)']))\n",
    "\n",
    "# Visual Comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_results, x='Recall (Catch Rate)', y='Model', palette='viridis')\n",
    "plt.title('Recall Comparison: Which model catches the most errors?')\n",
    "plt.xlabel('Recall Score (0.0 - 1.0)')\n",
    "plt.xlim(0, 1.0)\n",
    "plt.axvline(0.80, color='red', linestyle='--', label='Target Recall (80%)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fa90cb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, recall_score, precision_score, f1_score\n",
    "\n",
    "print(\"--- STARTING GRAND MODEL TOURNAMENT (Binary Classification) ---\")\n",
    "\n",
    "# ==========================================\n",
    "# 0. PREPARE TARGET (The Binary Conversion)\n",
    "# ==========================================\n",
    "# Business Rule: If Reliability < 0.95, it is a RISK (1). Otherwise, SAFE (0).\n",
    "THRESHOLD = 0.95\n",
    "target_name = 'Wages Reliability'\n",
    "\n",
    "print(f\"Converting {target_name} to Binary Risk (Threshold < {THRESHOLD})...\")\n",
    "y_binary = (df[target_name] < THRESHOLD).astype(int)\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "print(y_binary.value_counts(normalize=True).rename({0: 'Safe', 1: 'Risk'}))\n",
    "\n",
    "# Prepare Features (Drop Targets and IDs)\n",
    "# Note: Ensure you define 'df' from previous cleaning steps\n",
    "ids_to_drop = ['Request ID', 'Case Id', 'Service Provider Id', 'Service Provider Contact Id', 'Claim ID']\n",
    "targets_to_drop = ['Annual Leave Reliability', 'Long Service Leave Reliability', 'Wages Reliability']\n",
    "X = df.drop(columns=ids_to_drop + targets_to_drop, errors='ignore')\n",
    "\n",
    "# Split Data (Stratify ensures we keep the same % of risks in test set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42, stratify=y_binary)\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATA SANITIZATION (Fixing LightGBM Error)\n",
    "# ==========================================\n",
    "def clean_col_names(df):\n",
    "    # Replaces spaces, slashes, etc. with underscores\n",
    "    return df.rename(columns=lambda x: re.sub(r'[^A-Za-z0-9_]+', '_', x))\n",
    "\n",
    "X_train = clean_col_names(X_train)\n",
    "X_test = clean_col_names(X_test)\n",
    "\n",
    "# Calculate Imbalance Ratio for Boosting Models\n",
    "ratio = float(np.sum(y_train == 0)) / np.sum(y_train == 1)\n",
    "print(f\"Imbalance Ratio: {ratio:.2f}x (Models will weight Risk cases heavier)\")\n",
    "\n",
    "# Store results here\n",
    "leaderboard = []\n",
    "\n",
    "# ==========================================\n",
    "# 2. XGBOOST (The Favorite)\n",
    "# ==========================================\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "xgb_model = XGBClassifier(n_estimators=500, learning_rate=0.05, max_depth=6, \n",
    "                          scale_pos_weight=ratio, random_state=42, n_jobs=-1)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_probs = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Test 3 Thresholds\n",
    "for thresh in [0.50, 0.75, 0.90]:\n",
    "    preds = (xgb_probs >= thresh).astype(int)\n",
    "    leaderboard.append({\n",
    "        'Model': f'XGBoost (Threshold {thresh})',\n",
    "        'Recall (Catch Rate)': recall_score(y_test, preds),\n",
    "        'Precision (Accuracy)': precision_score(y_test, preds),\n",
    "        'F1-Score': f1_score(y_test, preds)\n",
    "    })\n",
    "\n",
    "# ==========================================\n",
    "# 3. LIGHTGBM (The Speedster)\n",
    "# ==========================================\n",
    "print(\"Training LightGBM...\")\n",
    "lgbm_model = LGBMClassifier(n_estimators=500, learning_rate=0.05, max_depth=6, \n",
    "                            scale_pos_weight=ratio, random_state=42, n_jobs=-1, verbose=-1)\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "lgbm_preds = lgbm_model.predict(X_test)\n",
    "\n",
    "leaderboard.append({\n",
    "    'Model': 'LightGBM (Default)',\n",
    "    'Recall (Catch Rate)': recall_score(y_test, lgbm_preds),\n",
    "    'Precision (Accuracy)': precision_score(y_test, lgbm_preds),\n",
    "    'F1-Score': f1_score(y_test, lgbm_preds)\n",
    "})\n",
    "\n",
    "# ==========================================\n",
    "# 4. DECISION TREE (The White Box)\n",
    "# ==========================================\n",
    "print(\"Training Decision Tree...\")\n",
    "dt_model = DecisionTreeClassifier(max_depth=4, class_weight='balanced', random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_preds = dt_model.predict(X_test)\n",
    "\n",
    "leaderboard.append({\n",
    "    'Model': 'Decision Tree (Depth 4)',\n",
    "    'Recall (Catch Rate)': recall_score(y_test, dt_preds),\n",
    "    'Precision (Accuracy)': precision_score(y_test, dt_preds),\n",
    "    'F1-Score': f1_score(y_test, dt_preds)\n",
    "})\n",
    "\n",
    "# ==========================================\n",
    "# 5. LOGISTIC REGRESSION (The Baseline)\n",
    "# ==========================================\n",
    "print(\"Training Logistic Regression...\")\n",
    "# Needs scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "log_model = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "log_model.fit(X_train_scaled, y_train)\n",
    "log_preds = log_model.predict(X_test_scaled)\n",
    "\n",
    "leaderboard.append({\n",
    "    'Model': 'Logistic Regression',\n",
    "    'Recall (Catch Rate)': recall_score(y_test, log_preds),\n",
    "    'Precision (Accuracy)': precision_score(y_test, log_preds),\n",
    "    'F1-Score': f1_score(y_test, log_preds)\n",
    "})\n",
    "\n",
    "# ==========================================\n",
    "# 6. FINAL RESULTS DISPLAY\n",
    "# ==========================================\n",
    "df_results = pd.DataFrame(leaderboard).sort_values('Recall (Catch Rate)', ascending=False)\n",
    "\n",
    "print(\"\\n--- üèÜ MODEL LEADERBOARD üèÜ ---\")\n",
    "display(df_results.style.background_gradient(cmap='Greens', subset=['Recall (Catch Rate)', 'Precision (Accuracy)']))\n",
    "\n",
    "# Visual Comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_results, x='Recall (Catch Rate)', y='Model', palette='viridis')\n",
    "plt.title('Recall Comparison: Which model catches the most errors?')\n",
    "plt.xlabel('Recall Score (0.0 - 1.0)')\n",
    "plt.xlim(0, 1.0)\n",
    "plt.axvline(0.80, color='red', linestyle='--', label='Target Recall (80%)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce02f8a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- üïµÔ∏è‚Äç‚ôÄÔ∏è LEAKAGE DETECTION: THE QUALIFYING ROUND üïµÔ∏è‚Äç‚ôÄÔ∏è ---\")\n",
    "\n",
    "# 1. PREPARE TIME-SERIES DATA\n",
    "# We need to fetch the Date column back from the raw dataset to sort chronologically\n",
    "# Assuming 'df' is your clean data and 'df_abs' (or df_raw) has the dates.\n",
    "# Let's try to find the date in df_main_raw (loaded in Cell 2)\n",
    "try:\n",
    "    # Create a temporary dataframe for testing\n",
    "    df_leakage = X.copy() # X is our feature set\n",
    "    \n",
    "    # Re-attach the target\n",
    "    df_leakage['Target_Risk'] = y_binary\n",
    "    \n",
    "    # Re-attach the Date (Critical Step)\n",
    "    # We assume df_main_raw was defined in Cell 2. If not, reload your CSV here.\n",
    "    df_leakage['Request_Date'] = pd.to_datetime(df_main_raw['Request Received Date'])\n",
    "    \n",
    "    # Sort by Date (Oldest -> Newest)\n",
    "    df_leakage = df_leakage.sort_values('Request_Date')\n",
    "    \n",
    "    print(\"‚úÖ Successfully sorted data chronologically.\")\n",
    "\n",
    "    # 2. STRICT TIME SPLIT (No Shuffling)\n",
    "    # Train on the Past (First 80%), Test on the Future (Last 20%)\n",
    "    cutoff = int(len(df_leakage) * 0.8)\n",
    "    \n",
    "    # Split Features and Target\n",
    "    X_time = df_leakage.drop(columns=['Target_Risk', 'Request_Date'])\n",
    "    y_time = df_leakage['Target_Risk']\n",
    "    \n",
    "    X_train_time = X_time.iloc[:cutoff]\n",
    "    y_train_time = y_time.iloc[:cutoff]\n",
    "    \n",
    "    X_test_time = X_time.iloc[cutoff:]\n",
    "    y_test_time = y_time.iloc[cutoff:]\n",
    "    \n",
    "    print(f\"Training on oldest {len(X_train_time)} claims.\")\n",
    "    print(f\"Testing on newest {len(X_test_time)} claims (The Future).\")\n",
    "\n",
    "    # 3. TRAIN TEST MODEL\n",
    "    # We use a quick XGBoost to see if it holds up\n",
    "    print(\"\\nTraining Time-Aware Model...\")\n",
    "    ratio_time = float(np.sum(y_train_time == 0)) / np.sum(y_train_time == 1)\n",
    "    \n",
    "    model_time = XGBClassifier(n_estimators=200, learning_rate=0.05, scale_pos_weight=ratio_time, random_state=42, n_jobs=-1)\n",
    "    model_time.fit(X_train_time, y_train_time)\n",
    "    \n",
    "    # 4. EVALUATE\n",
    "    y_pred_time = model_time.predict(X_test_time)\n",
    "    \n",
    "    print(\"\\n--- ‚è≥ TIME-TRAVEL CHECK RESULTS ‚è≥ ---\")\n",
    "    print(classification_report(y_test_time, y_pred_time, target_names=['Safe', 'Risk']))\n",
    "    \n",
    "    # 5. VERDICT\n",
    "    rec_score = recall_score(y_test_time, y_pred_time)\n",
    "    if rec_score < 0.60:\n",
    "        print(\"‚ùå FAILURE: Recall dropped significantly on future data. You likely have Time-Based Leakage.\")\n",
    "    elif rec_score < 0.80:\n",
    "        print(\"‚ö†Ô∏è CAUTION: Performance dropped slightly. This is normal, but check your features.\")\n",
    "    else:\n",
    "        print(\"‚úÖ PASS: Model generalizes well to the future. You are ready for the Tournament!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not run Time Check. Error: {e}\")\n",
    "    print(\"Did you reload 'df_main_raw' in Cell 2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7184a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "\n",
    "print(\"--- üïµÔ∏è‚Äç‚ôÄÔ∏è LEAKAGE DETECTION: THE QUALIFYING ROUND üïµÔ∏è‚Äç‚ôÄÔ∏è ---\")\n",
    "\n",
    "# 1. DEFINE TARGET & FEATURES (Locally for this test)\n",
    "# We recreate X and y here because they haven't been defined in the notebook yet\n",
    "THRESHOLD = 0.95\n",
    "target_col = 'Wages Reliability'\n",
    "\n",
    "# Create Binary Target\n",
    "y_check = (df[target_col] < THRESHOLD).astype(int)\n",
    "\n",
    "# Create Features (Drop Targets and potential IDs if they still exist)\n",
    "cols_to_exclude = ['Annual Leave Reliability', 'Long Service Leave Reliability', 'Wages Reliability',\n",
    "                   'Request ID', 'Case Id', 'Service Provider Id', 'Service Provider Contact Id', 'Claim ID']\n",
    "existing_exclude = [c for c in cols_to_exclude if c in df.columns]\n",
    "X_check = df.drop(columns=existing_exclude)\n",
    "\n",
    "# 2. RECOVER THE DATE (The \"Join\" Step)\n",
    "# We need to fetch the 'Request Received Date' from the RAW dataframe.\n",
    "# We use .loc[df.index] to ensure we only get dates for the rows that survived data cleaning.\n",
    "try:\n",
    "    # Make sure df_main_raw is available (from Cell 2)\n",
    "    if 'df_main_raw' not in locals():\n",
    "        raise ValueError(\"df_main_raw is missing! Please re-run Cell 2.\")\n",
    "\n",
    "    # Create a temporary dataframe for sorting\n",
    "    df_leakage = X_check.copy()\n",
    "    df_leakage['Target_Risk'] = y_check\n",
    "    \n",
    "    # CRITICAL: Fetch the date using the index to match rows perfectly\n",
    "    # Change 'Request Received Date' below if your raw column has a different name\n",
    "    date_col_name = 'Request Received Date' \n",
    "    df_leakage['Request_Date'] = df_main_raw.loc[df.index, date_col_name]\n",
    "    \n",
    "    # Convert to datetime just in case\n",
    "    df_leakage['Request_Date'] = pd.to_datetime(df_leakage['Request_Date'], errors='coerce')\n",
    "    \n",
    "    # Drop rows where date is missing (can't sort them)\n",
    "    df_leakage = df_leakage.dropna(subset=['Request_Date'])\n",
    "    \n",
    "    # 3. SORT CHRONOLOGICALLY (Oldest -> Newest)\n",
    "    df_leakage = df_leakage.sort_values('Request_Date')\n",
    "    print(f\"‚úÖ Successfully sorted {len(df_leakage)} claims by Date.\")\n",
    "\n",
    "    # 4. STRICT TIME SPLIT (First 80% vs Last 20%)\n",
    "    cutoff = int(len(df_leakage) * 0.8)\n",
    "    \n",
    "    # Split Features (drop the temp date column) and Target\n",
    "    X_time = df_leakage.drop(columns=['Target_Risk', 'Request_Date'])\n",
    "    y_time = df_leakage['Target_Risk']\n",
    "    \n",
    "    X_train_time = X_time.iloc[:cutoff]\n",
    "    y_train_time = y_time.iloc[:cutoff]\n",
    "    \n",
    "    X_test_time = X_time.iloc[cutoff:]\n",
    "    y_test_time = y_time.iloc[cutoff:]\n",
    "    \n",
    "    print(f\"Training on Oldest {len(X_train_time)} claims (Past).\")\n",
    "    print(f\"Testing on Newest {len(X_test_time)} claims (Future).\")\n",
    "\n",
    "    # 5. TRAIN & TEST\n",
    "    print(\"\\nTraining Time-Aware Model (XGBoost)...\")\n",
    "    # Calculate class weight for this"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
