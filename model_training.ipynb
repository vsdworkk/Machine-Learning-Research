{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b02331",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install XGBoost if you haven't already (uncomment if needed)\n",
    "# !pip install xgboost\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"--- Configuring Model Training ---\")\n",
    "\n",
    "# 1. DEFINE LEAKY COLUMNS TO DROP\n",
    "# These are the IDs causing the model to \"memorize\" instead of learn\n",
    "id_cols_to_drop = [\n",
    "    'Request ID', \n",
    "    'Case Id', \n",
    "    'Service Provider Id', \n",
    "    'Service Provider Contact Id',\n",
    "    'Claim ID' # Just in case\n",
    "]\n",
    "\n",
    "# 2. PREPARE X and y\n",
    "target_name = 'Wages Reliability'\n",
    "targets_to_exclude = ['Annual Leave Reliability', 'Long Service Leave Reliability', 'Wages Reliability']\n",
    "\n",
    "# Drop Targets AND the IDs\n",
    "cols_to_drop = targets_to_exclude + id_cols_to_drop\n",
    "# Only drop what actually exists\n",
    "existing_drop = [c for c in cols_to_drop if c in df.columns]\n",
    "\n",
    "X = df.drop(columns=existing_drop)\n",
    "y = df[target_name]\n",
    "\n",
    "# 3. SPLIT DATA\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training Features: {X_train.shape[1]} columns\")\n",
    "print(f\"Training Rows: {X_train.shape[0]}\")\n",
    "\n",
    "# 4. TRAIN XGBOOST (The Upgrade)\n",
    "# n_estimators=500: More trees\n",
    "# learning_rate=0.05: Learns slower but more accurately\n",
    "# n_jobs=-1: Uses all CPU cores\n",
    "model = XGBRegressor(\n",
    "    n_estimators=500, \n",
    "    learning_rate=0.05, \n",
    "    max_depth=6, \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost... (This might take 30 seconds)\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. EVALUATE\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n--- Final Model Performance ---\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R2 Score (Accuracy):       {r2:.4f}\")\n",
    "\n",
    "# 6. PLOT FEATURE IMPORTANCE (The \"Clean\" View)\n",
    "# Now that IDs are gone, what REALLY matters?\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance, palette='magma')\n",
    "plt.title(f'True Drivers of {target_name} (IDs Removed)')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7fe85c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. SELECT FEATURES & TARGET\n",
    "# The Target is what we want to predict\n",
    "target_name = 'Wages Reliability'\n",
    "\n",
    "# The Features (X) are everything EXCEPT the target columns\n",
    "# We drop ALL reliability targets so the model doesn't \"cheat\" by looking at Annual Leave to predict Wages\n",
    "drop_targets = ['Annual Leave Reliability', 'Long Service Leave Reliability', 'Wages Reliability']\n",
    "X = df.drop(columns=drop_targets, errors='ignore')\n",
    "y = df[target_name]\n",
    "\n",
    "# 2. SPLIT DATA (80% Training, 20% Testing)\n",
    "# random_state=42 ensures we get the same split every time (reproducibility)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training on {X_train.shape[0]} rows. Testing on {X_test.shape[0]} rows.\")\n",
    "\n",
    "# 3. TRAIN THE MODEL\n",
    "# n_estimators=100 means \"build 100 decision trees\"\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"âœ… Model Training Complete.\")\n",
    "\n",
    "# 4. EVALUATE PERFORMANCE\n",
    "# Make predictions on the Test Set (data the model has never seen)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n--- Model Performance Results ---\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R2 Score (Accuracy):       {r2:.4f}\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Interpretation: On average, the model's reliability score is off by {mae*100:.2f}%.\")\n",
    "\n",
    "# 5. VISUALIZE FEATURE IMPORTANCE (The \"Why\")\n",
    "# This shows which columns drove the decisions the most\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance, palette='viridis')\n",
    "plt.title(f'Top 10 Predictors for {target_name}')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
