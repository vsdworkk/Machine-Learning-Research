{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b02331",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install XGBoost if you haven't already (uncomment if needed)\n",
    "# !pip install xgboost\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"--- Configuring Model Training ---\")\n",
    "\n",
    "# 1. DEFINE LEAKY COLUMNS TO DROP\n",
    "# These are the IDs causing the model to \"memorize\" instead of learn\n",
    "id_cols_to_drop = [\n",
    "    'Request ID', \n",
    "    'Case Id', \n",
    "    'Service Provider Id', \n",
    "    'Service Provider Contact Id',\n",
    "    'Claim ID' # Just in case\n",
    "]\n",
    "\n",
    "# 2. PREPARE X and y\n",
    "target_name = 'Wages Reliability'\n",
    "targets_to_exclude = ['Annual Leave Reliability', 'Long Service Leave Reliability', 'Wages Reliability']\n",
    "\n",
    "# Drop Targets AND the IDs\n",
    "cols_to_drop = targets_to_exclude + id_cols_to_drop\n",
    "# Only drop what actually exists\n",
    "existing_drop = [c for c in cols_to_drop if c in df.columns]\n",
    "\n",
    "X = df.drop(columns=existing_drop)\n",
    "y = df[target_name]\n",
    "\n",
    "# 3. SPLIT DATA\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training Features: {X_train.shape[1]} columns\")\n",
    "print(f\"Training Rows: {X_train.shape[0]}\")\n",
    "\n",
    "# 4. TRAIN XGBOOST (The Upgrade)\n",
    "# n_estimators=500: More trees\n",
    "# learning_rate=0.05: Learns slower but more accurately\n",
    "# n_jobs=-1: Uses all CPU cores\n",
    "model = XGBRegressor(\n",
    "    n_estimators=500, \n",
    "    learning_rate=0.05, \n",
    "    max_depth=6, \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost... (This might take 30 seconds)\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. EVALUATE\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n--- Final Model Performance ---\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R2 Score (Accuracy):       {r2:.4f}\")\n",
    "\n",
    "# 6. PLOT FEATURE IMPORTANCE (The \"Clean\" View)\n",
    "# Now that IDs are gone, what REALLY matters?\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance, palette='magma')\n",
    "plt.title(f'True Drivers of {target_name} (IDs Removed)')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7fe85c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. SELECT FEATURES & TARGET\n",
    "# The Target is what we want to predict\n",
    "target_name = 'Wages Reliability'\n",
    "\n",
    "# The Features (X) are everything EXCEPT the target columns\n",
    "# We drop ALL reliability targets so the model doesn't \"cheat\" by looking at Annual Leave to predict Wages\n",
    "drop_targets = ['Annual Leave Reliability', 'Long Service Leave Reliability', 'Wages Reliability']\n",
    "X = df.drop(columns=drop_targets, errors='ignore')\n",
    "y = df[target_name]\n",
    "\n",
    "# 2. SPLIT DATA (80% Training, 20% Testing)\n",
    "# random_state=42 ensures we get the same split every time (reproducibility)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training on {X_train.shape[0]} rows. Testing on {X_test.shape[0]} rows.\")\n",
    "\n",
    "# 3. TRAIN THE MODEL\n",
    "# n_estimators=100 means \"build 100 decision trees\"\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"✅ Model Training Complete.\")\n",
    "\n",
    "# 4. EVALUATE PERFORMANCE\n",
    "# Make predictions on the Test Set (data the model has never seen)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n--- Model Performance Results ---\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R2 Score (Accuracy):       {r2:.4f}\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Interpretation: On average, the model's reliability score is off by {mae*100:.2f}%.\")\n",
    "\n",
    "# 5. VISUALIZE FEATURE IMPORTANCE (The \"Why\")\n",
    "# This shows which columns drove the decisions the most\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance, palette='viridis')\n",
    "plt.title(f'Top 10 Predictors for {target_name}')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2876d8a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Diagnostic Cell: Where did the features go?\n",
    "\n",
    "print(\"--- Checking for Key Features in Training Data ---\")\n",
    "\n",
    "# List of the 'Smart' features we expect to see\n",
    "key_features = [\n",
    "    'Provider_Reputation_Score', \n",
    "    'IP_Wage_to_ABS_Ratio', \n",
    "    'IP_Tenure_Years', \n",
    "    'IP Wages'\n",
    "]\n",
    "\n",
    "# 1. Do they exist?\n",
    "for col in key_features:\n",
    "    if col in X_train.columns:\n",
    "        print(f\"✅ FOUND: {col}\")\n",
    "    else:\n",
    "        print(f\"❌ MISSING: {col} (This is why it's not in the chart!)\")\n",
    "\n",
    "# 2. If they exist, how weak are they?\n",
    "# Let's look at their correlation with the Target\n",
    "if 'Wages Reliability' in df.columns:\n",
    "    print(\"\\n--- Correlation with Target ---\")\n",
    "    # We check the original df because X_train is already split\n",
    "    for col in key_features:\n",
    "        if col in df.columns:\n",
    "            corr = df[col].corr(df['Wages Reliability'])\n",
    "            print(f\"{col}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144215e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "print(\"--- Pivoting to Binary Classification ---\")\n",
    "\n",
    "# 1. DEFINE THE THRESHOLD (Business Logic)\n",
    "# User Proposal: Anything below 0.95 is \"Unreliable\" (Risk)\n",
    "THRESHOLD = 0.95\n",
    "\n",
    "# Create Binary Target: 1 = RISK (Unreliable), 0 = SAFE (Reliable)\n",
    "# We usually define the \"Positive Class\" (1) as the thing we want to catch (The Risk)\n",
    "y_class = (df['Wages Reliability'] < THRESHOLD).astype(int)\n",
    "\n",
    "print(f\"Reliability Threshold: {THRESHOLD*100}%\")\n",
    "print(\"Class Distribution:\")\n",
    "print(y_class.value_counts(normalize=True).rename({0: 'Safe (Majority)', 1: 'Risk (Minority)'}))\n",
    "\n",
    "# 2. SPLIT DATA\n",
    "# We use the same X features as before (IDs removed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_class, test_size=0.2, random_state=42, stratify=y_class)\n",
    "# Note: stratify=y_class ensures we have the same % of bad cases in train and test\n",
    "\n",
    "# 3. CALCULATE SCALE_POS_WEIGHT\n",
    "# This tells XGBoost: \"Pay X times more attention to the minority class\"\n",
    "# Formula: Count(Majority) / Count(Minority)\n",
    "ratio = float(np.sum(y_train == 0)) / np.sum(y_train == 1)\n",
    "print(f\"\\nImbalance Ratio: {ratio:.2f} (Model will weight 'Risk' cases {ratio:.2f}x more)\")\n",
    "\n",
    "# 4. TRAIN CLASSIFIER\n",
    "model_class = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    scale_pos_weight=ratio, # <--- CRITICAL FIX FOR IMBALANCE\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "model_class.fit(X_train, y_train)\n",
    "\n",
    "# 5. EVALUATE (Confusion Matrix)\n",
    "y_pred_class = model_class.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred_class, target_names=['Safe', 'Risk']))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(6, 5))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_class, display_labels=['Safe', 'Risk'], cmap='Blues', colorbar=False)\n",
    "plt.title(\"Confusion Matrix (Did we catch the risks?)\")\n",
    "plt.show()\n",
    "\n",
    "# 6. CHECK FEATURE IMPORTANCE (Again)\n",
    "# Does the logic change when looking for anomalies?\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model_class.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance, palette='coolwarm')\n",
    "plt.title(f'Top Predictors of UNRELIABILITY (< {THRESHOLD})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2cd5a7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# 1. Get the Probability Scores (0% to 100% risk) instead of just Yes/No\n",
    "y_probs = model_class.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2. Test different Thresholds\n",
    "thresholds = [0.50, 0.60, 0.70, 0.80, 0.85, 0.90, 0.95]\n",
    "\n",
    "print(f\"{'Threshold':<10} | {'Precision (Accuracy of Flags)':<30} | {'Recall (Risks Caught)':<25}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for t in thresholds:\n",
    "    # Apply the new threshold\n",
    "    y_pred_new = (y_probs >= t).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    report = classification_report(y_test, y_pred_new, output_dict=True)\n",
    "    prec = report['1']['precision']\n",
    "    rec = report['1']['recall']\n",
    "    \n",
    "    print(f\"{t:.2f}       | {prec:.2%}                         | {rec:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4605c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visual Proof of \"No Leakage\"\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# We sample 2000 points just to keep the plot readable\n",
    "sns.scatterplot(data=df.sample(2000, random_state=42), x='IP Wages', y='Wages Reliability', alpha=0.3)\n",
    "\n",
    "plt.title(\"Leakage Test: IP Wage vs. Reliability\")\n",
    "plt.xlabel(\"IP Wage ($)\")\n",
    "plt.ylabel(\"Reliability Score (0.0 - 1.0)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
