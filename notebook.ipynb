{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d10a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # Added for quick visual checks\n",
    "\n",
    "# Set pandas to display all columns so you can see your changes\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d04d30",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# REPLACE these filenames with your actual file paths\n",
    "df = pd.read_csv('FEG_Main_Dataset.csv') \n",
    "df_abs = pd.read_csv('ABS_Industry_Wages.csv') \n",
    "\n",
    "print(f\"Main Dataset Shape: {df.shape}\")\n",
    "print(f\"ABS Dataset Shape: {df_abs.shape}\")\n",
    "\n",
    "# Debug: Check the data types. \n",
    "# If 'IP Weekly Wage' says 'object', that is why your code failed earlier.\n",
    "print(\"\\n--- Data Types Check ---\")\n",
    "print(df[['IP Weekly Wage', 'Industry']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2247a49",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Clean Main Dataset 'IP Weekly Wage'\n",
    "# Convert to string -> remove '$' and ',' -> Convert to Numeric\n",
    "df['IP Weekly Wage'] = df['IP Weekly Wage'].astype(str).str.replace(r'[$,]', '', regex=True)\n",
    "df['IP Weekly Wage'] = pd.to_numeric(df['IP Weekly Wage'], errors='coerce')\n",
    "\n",
    "# 2. Clean ABS Dataset Wage Column\n",
    "# We do the same thing here to ensure we can divide them later\n",
    "if 'ABS_Average_Weekly_Wage' in df_abs.columns:\n",
    "    df_abs['ABS_Average_Weekly_Wage'] = df_abs['ABS_Average_Weekly_Wage'].astype(str).str.replace(r'[$,]', '', regex=True)\n",
    "    df_abs['ABS_Average_Weekly_Wage'] = pd.to_numeric(df_abs['ABS_Average_Weekly_Wage'], errors='coerce')\n",
    "\n",
    "# Sanity Check: Did it work?\n",
    "print(\"New Data Type for IP Weekly Wage:\", df['IP Weekly Wage'].dtype)\n",
    "print(\"Number of NaN wages (unparseable):\", df['IP Weekly Wage'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc170fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the target columns\n",
    "target_cols = ['Annual Leave Reliability', 'Long Service Leave Reliability', 'Wages Reliability']\n",
    "\n",
    "print(\"--- Cleaning Targets ---\")\n",
    "for col in target_cols:\n",
    "    if col in df.columns:\n",
    "        # 1. String Cleaning: Force to string, remove '%' and whitespace\n",
    "        # This turns \" 95% \" into \"95\"\n",
    "        df[col] = df[col].astype(str).str.replace('%', '', regex=False).str.strip()\n",
    "        \n",
    "        # 2. Convert to Numeric\n",
    "        # 'errors=coerce' turns garbage (like \"N/A\" or \"Error\") into proper NaN\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # 3. Scale Correction (Crucial Check)\n",
    "        # If the data was \"85.5\", it is now 85.5. But we want 0.0 to 1.0.\n",
    "        # Logic: If the max value is > 1.0, it implies 0-100 scale, so we divide by 100.\n",
    "        if df[col].max() > 1.0:\n",
    "            print(f\"Detected 0-100 scale for {col}. Dividing by 100...\")\n",
    "            df[col] = df[col] / 100.0\n",
    "            \n",
    "        # 4. Clip values to ensure strict 0.0 - 1.0 range\n",
    "        # (Handles negatives by turning them to 0.0)\n",
    "        df[col] = df[col].clip(lower=0.0, upper=1.0)\n",
    "        \n",
    "        # 5. Drop rows where the Target is still NaN (cannot train on them)\n",
    "        initial_len = len(df)\n",
    "        df = df.dropna(subset=[col])\n",
    "        dropped = initial_len - len(df)\n",
    "        if dropped > 0:\n",
    "            print(f\"Dropped {dropped} rows with invalid/missing {col}\")\n",
    "\n",
    "# Final Sanity Check\n",
    "print(\"\\n--- Final Target Stats (Should be 0.0 to 1.0) ---\")\n",
    "print(df[target_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeb2ebd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Internal Feature Engineering (Tenure) - CORRECTED\n",
    "\n",
    "# 1. Convert to datetime with Australian format (dayfirst=True)\n",
    "# 'errors=coerce' handles garbage text, 'dayfirst=True' handles the 13/01/2023 issue\n",
    "df['IP Commencement Date'] = pd.to_datetime(df['IP Commencement Date'], dayfirst=True, errors='coerce')\n",
    "df['IP Termination Date'] = pd.to_datetime(df['IP Termination Date'], dayfirst=True, errors='coerce')\n",
    "\n",
    "# 2. Calculate Tenure in Years\n",
    "df['IP_Tenure_Years'] = (df['IP Termination Date'] - df['IP Commencement Date']).dt.days / 365.25\n",
    "\n",
    "# 3. Fix Logic: Negative tenure -> 0\n",
    "df.loc[df['IP_Tenure_Years'] < 0, 'IP_Tenure_Years'] = 0\n",
    "\n",
    "# 4. Fix Missing: Fill NaN with -1\n",
    "df['IP_Tenure_Years'] = df['IP_Tenure_Years'].fillna(-1)\n",
    "\n",
    "# Sanity Check: Look at the distribution\n",
    "print(df['IP_Tenure_Years'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825e0f2d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if 'Industry' in df.columns:\n",
    "    # Merge\n",
    "    df = df.merge(df_abs[['Industry', 'ABS_Average_Weekly_Wage']], on='Industry', how='left')\n",
    "    \n",
    "    # Fill missing ABS wages with National Median\n",
    "    national_median = df_abs['ABS_Average_Weekly_Wage'].median()\n",
    "    df['ABS_Average_Weekly_Wage'] = df['ABS_Average_Weekly_Wage'].fillna(national_median)\n",
    "    \n",
    "    # Create Ratio Feature (Float divided by Float)\n",
    "    df['IP_Wage_to_ABS_Ratio'] = df['IP Weekly Wage'] / (df['ABS_Average_Weekly_Wage'] + 1e-5)\n",
    "    \n",
    "    # Create High Risk Flag\n",
    "    df['Flag_High_Wage_Deviation'] = (df['IP_Wage_to_ABS_Ratio'] > 1.5).astype(int)\n",
    "    \n",
    "    # Drop the raw dollar value\n",
    "    df.drop(columns=['ABS_Average_Weekly_Wage'], inplace=True)\n",
    "\n",
    "# Sanity Check: Show the new columns\n",
    "print(df[['Industry', 'IP Weekly Wage', 'IP_Wage_to_ABS_Ratio']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b71a269",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    # Claimant Data\n",
    "    'Claim ID', 'Claim Type', 'Claim Form Received Date', 'Claimant Age', \n",
    "    'Service Years At Appointment', 'Job Title', 'Job Duty Description', \n",
    "    'Claimant Confident of Amounts Owed', 'Information Held About Owed Entitlements',\n",
    "    'Claimant Commencement Date', 'Claimant Termination Date', \n",
    "    'Claimant Weekly Wage', 'Claimant Annual Leave', 'Claimant Wages',\n",
    "    \n",
    "    # CM Recommended (Leakage)\n",
    "    'CM Recommended Employment Type', 'CM Recommended Weekly Wage', \n",
    "    'CM Recommended Commencement Date', 'CM Recommended Termination Date',\n",
    "    'CM Recommended Annual Leave', 'CM Recommended Long Service Leave', \n",
    "    'CM Recommended Wages',\n",
    "    \n",
    "    # Raw IP Dates (We have Tenure now)\n",
    "    'IP Commencement Date', 'IP Termination Date'\n",
    "]\n",
    "\n",
    "# Only drop cols that exist\n",
    "existing_cols = [c for c in cols_to_drop if c in df.columns]\n",
    "df.drop(columns=existing_cols, inplace=True)\n",
    "\n",
    "# Sanity Check: Check if any 'CM Recommended' columns remain\n",
    "leaking = [c for c in df.columns if 'CM Recommended' in c]\n",
    "print(\"Leaking columns remaining (Should be empty):\", leaking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b35d62c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 8: Final Cleanup (Corrected Order)\n",
    "\n",
    "days_col = 'Days Between IP Verified Data Request and Received Date'\n",
    "\n",
    "if days_col in df.columns:\n",
    "    print(f\"Rows before cleanup: {len(df)}\")\n",
    "    \n",
    "    # 1. Handle Negatives (Don't drop the row, just mark the value as NaN)\n",
    "    # Logic: Negative time is impossible, so it's a data error.\n",
    "    mask_negatives = df[days_col] < 0\n",
    "    print(f\"Found {mask_negatives.sum()} negative values. Setting them to NaN.\")\n",
    "    df.loc[mask_negatives, days_col] = np.nan\n",
    "    \n",
    "    # 2. Impute Missing Values (Crucial Step to save the 5,000 rows)\n",
    "    # We calculate the median based on the valid positive numbers\n",
    "    median_days = df[days_col].median()\n",
    "    print(f\"Imputing missing days with Median: {median_days}\")\n",
    "    \n",
    "    # Add a \"Was Missing\" flag so the model knows this was imputed\n",
    "    df[f'{days_col}_missing'] = df[days_col].isna().astype(int)\n",
    "    df[days_col] = df[days_col].fillna(median_days)\n",
    "    \n",
    "    # 3. Winsorize (Cap extreme highs)\n",
    "    # Now that we have full data, we cap the top 1%\n",
    "    limit = df[days_col].quantile(0.99)\n",
    "    print(f\"Capping extreme values at: {limit}\")\n",
    "    df.loc[df[days_col] > limit, days_col] = limit\n",
    "\n",
    "# 4. General Imputation for all other columns\n",
    "for col in df.columns:\n",
    "    if df[col].isna().sum() > 0:\n",
    "        # Create indicator\n",
    "        df[f'{col}_missing'] = df[col].isna().astype(int)\n",
    "        \n",
    "        # Fill values\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        else:\n",
    "            mode_val = df[col].mode()[0] if not df[col].mode().empty else 'Unknown'\n",
    "            df[col] = df[col].fillna(mode_val)\n",
    "\n",
    "print(f\"Final Data Shape: {df.shape}\")\n",
    "print(\"Cleaning Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7671ac8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 9: Service Provider Reputation Feature\n",
    "\n",
    "# 1. Define the ID and the Target we want to use as a proxy for \"Reputation\"\n",
    "provider_col = 'Service Provider Id'\n",
    "target_col = 'Wages Reliability' # Using Wages as the general signal for reliability\n",
    "\n",
    "if provider_col in df.columns and target_col in df.columns:\n",
    "    print(\"--- Engineering Service Provider Reputation ---\")\n",
    "    \n",
    "    # Ensure ID is treated as a category, not a number\n",
    "    df[provider_col] = df[provider_col].astype(str)\n",
    "\n",
    "    # Calculate Global Mean (Baseline for new/unknown providers)\n",
    "    global_mean = df[target_col].mean()\n",
    "    \n",
    "    # Calculate Sum and Count per Provider\n",
    "    grouped = df.groupby(provider_col)[target_col]\n",
    "    provider_sum = grouped.transform('sum')\n",
    "    provider_count = grouped.transform('count')\n",
    "    \n",
    "    # --- LEAVE-ONE-OUT CALCULATION ---\n",
    "    # Formula: (Sum of all their scores - Current Row's Score) / (Total Count - 1)\n",
    "    # This ensures the model can't \"cheat\" by seeing the answer in the input\n",
    "    df['Provider_Reputation_Score'] = (provider_sum - df[target_col]) / (provider_count - 1)\n",
    "    \n",
    "    # Handle Edge Cases:\n",
    "    # 1. If a provider has only 1 case, (Count - 1) is 0 -> Division by Zero -> NaN/Inf\n",
    "    # 2. We fill these gaps with the Global Mean (innocent until proven guilty)\n",
    "    df['Provider_Reputation_Score'] = df['Provider_Reputation_Score'].fillna(global_mean)\n",
    "    # Also handle explicit infinity if numpy generated it\n",
    "    df['Provider_Reputation_Score'] = df['Provider_Reputation_Score'].replace([np.inf, -np.inf], global_mean)\n",
    "    \n",
    "    # Drop the raw ID columns (The model uses the score now)\n",
    "    df.drop(columns=[provider_col, 'Service Provider Contact Id'], inplace=True, errors='ignore')\n",
    "    \n",
    "    print(f\"Feature Created: 'Provider_Reputation_Score'\")\n",
    "    print(f\"Global Mean Reliability: {global_mean:.4f}\")\n",
    "    print(df[['Provider_Reputation_Score', target_col]].head())\n",
    "\n",
    "else:\n",
    "    print(f\"Skipping: {provider_col} or {target_col} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5558c2f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 10: Final Pre-Flight Check\n",
    "\n",
    "print(\"--- Final Dataset Status ---\")\n",
    "\n",
    "# 1. Check for Non-Numeric Columns\n",
    "# (Should only be empty or maybe 'Industry' if you want to One-Hot Encode it later)\n",
    "non_numerics = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "if len(non_numerics) > 0:\n",
    "    print(f\"⚠️ WARNING: The following columns are still text: {non_numerics}\")\n",
    "    print(\"Action: You will need to One-Hot Encode these (e.g. pd.get_dummies) before training.\")\n",
    "else:\n",
    "    print(\"✅ SUCCESS: All columns are numeric.\")\n",
    "\n",
    "# 2. Check for Missing Values\n",
    "nan_counts = df.isna().sum().sum()\n",
    "if nan_counts > 0:\n",
    "    print(f\"❌ CRITICAL: {nan_counts} missing values remain.\")\n",
    "    # Show which columns have them\n",
    "    print(df.columns[df.isna().any()].tolist())\n",
    "else:\n",
    "    print(\"✅ SUCCESS: No missing values found.\")\n",
    "\n",
    "# 3. Check Feature Power (Correlation)\n",
    "# Does our new 'Reputation' feature actually work?\n",
    "if 'Provider_Reputation_Score' in df.columns and 'Wages Reliability' in df.columns:\n",
    "    corr = df['Provider_Reputation_Score'].corr(df['Wages Reliability'])\n",
    "    print(f\"\\nCorrelation between Provider Reputation & Reliability: {corr:.4f}\")\n",
    "    if corr > 0.1:\n",
    "        print(\"✅ STRONG SIGNAL: Past behavior predicts future reliability.\")\n",
    "    elif corr < 0.05:\n",
    "        print(\"⚠️ WEAK SIGNAL: Provider history might not be predictive in this dataset.\")\n",
    "\n",
    "print(f\"\\nFinal Shape for Training: {df.shape}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
